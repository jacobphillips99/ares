{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "from sqlalchemy.orm import Session\n",
    "\n",
    "from ares.configs.base import Rollout\n",
    "from ares.configs.pydantic_sql_helpers import recreate_model\n",
    "from ares.databases.embedding_database import (\n",
    "    BASE_EMBEDDING_DB_PATH,\n",
    "    TEST_EMBEDDING_DB_PATH,\n",
    "    FaissIndex,\n",
    "    IndexManager,\n",
    ")\n",
    "from ares.databases.structured_database import (\n",
    "    TEST_ROBOT_DB_PATH,\n",
    "    RolloutSQLModel,\n",
    "    setup_database,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ares.databases.embedding_database import IndexManager, FaissIndex, TEST_EMBEDDING_DB_PATH\n",
    "\n",
    "index_manager = IndexManager(TEST_EMBEDDING_DB_PATH, FaissIndex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vecs = index_manager.get_all_matrices()\n",
    "\n",
    "for name, vecs in all_vecs.items():\n",
    "    print(name, vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_dataset_statistics(arr: np.ndarray):\n",
    "    # get mean and std for each last dimension\n",
    "    # each is n_samples x n_timesteps x n_dims\n",
    "    assert len(arr.shape) == 3\n",
    "    return {\n",
    "        'mean': np.mean(arr, axis=(0, 1)),\n",
    "        'std': np.std(arr, axis=(0, 1)),\n",
    "        'min': np.min(arr, axis=(0, 1)),\n",
    "        'max': np.max(arr, axis=(0, 1)),\n",
    "        'q01': np.percentile(arr, 1, axis=(0, 1)),\n",
    "        'q99': np.percentile(arr, 99, axis=(0, 1)),\n",
    "        'num_transitions': arr.shape[0] * arr.shape[1],\n",
    "        'num_trajectories': arr.shape[0],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = {name: get_dataset_statistics(vecs) for name, vecs in all_vecs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import numpy as np\n",
    "# taken from openvla/prismatic https://github.com/openvla/openvla/blob/main/prismatic/vla/datasets/rlds/utils/data_utils.py\n",
    "\n",
    "class NormalizationType(str, Enum):\n",
    "    NORMAL = \"normal\"               # Normalize to Mean = 0, Stdev = 1\n",
    "    BOUNDS = \"bounds\"               # Normalize to Interval = [-1, 1]\n",
    "    BOUNDS_Q99 = \"bounds_q99\"       # Normalize [quantile_01, ..., quantile_99] --> [-1, ..., 1]\n",
    "      \n",
    "\n",
    "def normalize_dataset(vecs: np.ndarray, stats: Dict[str, float], normalization_type: NormalizationType = NormalizationType.NORMAL):\n",
    "    if normalization_type == NormalizationType.NORMAL:\n",
    "        normed_vecs = (vecs - stats['mean']) / stats['std']\n",
    "    elif normalization_type == NormalizationType.BOUNDS:\n",
    "        normed_vecs = (vecs - stats['min']) / (stats['max'] - stats['min'])\n",
    "    elif normalization_type == NormalizationType.BOUNDS_Q99:\n",
    "        normalized = (vecs - stats['q01']) / (stats['q99'] - stats['q01'])\n",
    "        normed_vecs = np.clip(normalized, -1, 1)\n",
    "    return 2*normed_vecs - 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_type = NormalizationType.BOUNDS_Q99\n",
    "normed_vecs = {name: normalize_dataset(vecs, stats, norm_type) for name, vecs, stats in zip(all_vecs.keys(), all_vecs.values(), norms.values())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_graphs = len(all_vecs)\n",
    "# Create n rows and 2 columns\n",
    "fig, axs = plt.subplots(\n",
    "    n_graphs, 2, figsize=(15, 5 * n_graphs),  # Height scales with number of distributions\n",
    "    gridspec_kw={'hspace': 0.5, 'wspace': 0.3}\n",
    ")\n",
    "\n",
    "for i, (name, vecs) in enumerate(all_vecs.items()):\n",
    "    # Plot unnormalized data (left column)\n",
    "    for j in range(vecs.shape[-1]):\n",
    "        axs[i][0].hist(vecs[:, :, j].flatten(), bins=100, label=f\"dim {j}\")\n",
    "        axs[i][0].set_title(f\"{name} (unnormalized)\")\n",
    "        axs[i][0].legend()\n",
    "    \n",
    "    # Plot normalized data (right column)\n",
    "    # these are in the range [-1, 1]\n",
    "    normed_vec = normed_vecs[name]\n",
    "    for j in range(normed_vec.shape[-1]):\n",
    "        axs[i][1].hist(normed_vec[:, :, j].flatten(), bins=100, label=f\"dim {j}\", range=(-1, 1))\n",
    "        axs[i][1].set_title(f\"{name} (normalized via {norm_type})\")\n",
    "        axs[i][1].legend()\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
