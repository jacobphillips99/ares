{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note that 0 FPS means just first and last frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potentially combine paths!\n",
    "paths = [\n",
    "    \"/workspaces/ares/data/eval_dump/eval_results_gpt-4o-mini_2025-01-06_21-54-15_video.csv\",\n",
    "    \"/workspaces/ares/data/eval_dump/eval_results_gpt-4o_2025-01-06_22-00-33_video.csv\",\n",
    "    \"/workspaces/ares/data/eval_dump/eval_results_gemini-1.5-pro_2025-01-06_22-04-12_video.csv\",\n",
    "    \"/workspaces/ares/data/eval_dump/eval_results_gemini-1.5-pro_2025-01-06_23-07-26_frame_descriptions.csv\",\n",
    "    \"/workspaces/ares/data/eval_dump/eval_results_gpt-4o_2025-01-07_00-33-35_frame_descriptions.csv\",\n",
    "    \"/workspaces/ares/data/eval_dump/eval_results_gpt-4o-mini_2025-01-07_01-11-20_frame_descriptions.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv(path) for path in paths])\n",
    "df['label'] = df['success_flag'].apply(lambda x: 1 if x== 'success' else 0)\n",
    "df['accuracy_of_mean'] = df['label'] == (df['mean_performance'] > 0.5)\n",
    "df['accuracy_of_median'] = df['label'] == (df['median_performance'] > 0.5)\n",
    "\n",
    "df['vote_str'] = df['performance']\n",
    "df['votes_float'] = df['performance'].apply(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in df.groupby(['vlm', 'method', 'fps']):\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{df['accuracy_of_mean'].mean():.3f}, {df['accuracy_of_median'].mean():.3f}, {df['votes_float'].apply(lambda x: len(x)).mean():.3f}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for computing means and std errors\n",
    "def mean_stderr(data):\n",
    "    mean = np.mean(data)\n",
    "    stderr = np.std(data) / np.sqrt(len(data))\n",
    "    return mean, stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by task, VLM, FPS, and method to calculate mean accuracy\n",
    "task_model_fps_results = df.groupby(['task', 'vlm', 'fps', 'method']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'accuracy': x['accuracy_of_mean'].mean(),\n",
    "        'std_err': x['accuracy_of_mean'].std() / np.sqrt(len(x))\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Get unique values\n",
    "tasks = task_model_fps_results['task'].unique()\n",
    "vlms = task_model_fps_results['vlm'].unique()\n",
    "fps_values = sorted(task_model_fps_results['fps'].unique())\n",
    "methods = task_model_fps_results['method'].unique()\n",
    "\n",
    "# Create subplot for each task\n",
    "fig, axes = plt.subplots(len(tasks), 1, figsize=(12, 5*len(tasks)))\n",
    "if len(tasks) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for task_idx, (task, ax) in enumerate(zip(tasks, axes)):\n",
    "    task_data = task_model_fps_results[task_model_fps_results['task'] == task]\n",
    "    x = np.arange(len(fps_values))\n",
    "    width = 0.8 / (len(vlms) * len(methods))  # Adjusted width for methods\n",
    "\n",
    "    # Plot bars for each VLM and method combination\n",
    "    bar_idx = 0\n",
    "    for vlm in vlms:\n",
    "        for method in methods:\n",
    "            # Calculate offset for this combination's bars\n",
    "            offset = (bar_idx - (len(vlms) * len(methods))/2 + 0.5) * width\n",
    "            \n",
    "            accuracies = []\n",
    "            errors = []\n",
    "            positions = []\n",
    "            \n",
    "            for j, fps in enumerate(fps_values):\n",
    "                combo_data = task_data[\n",
    "                    (task_data['vlm'] == vlm) & \n",
    "                    (task_data['fps'] == fps) & \n",
    "                    (task_data['method'] == method)\n",
    "                ]\n",
    "                if not combo_data.empty:\n",
    "                    positions.append(j + offset)\n",
    "                    accuracies.append(combo_data['accuracy'].iloc[0])\n",
    "                    errors.append(combo_data['std_err'].iloc[0])\n",
    "            \n",
    "            if positions:  # Only plot if we have data\n",
    "                ax.bar(positions, accuracies, width, \n",
    "                       label=f'{vlm} ({method})',\n",
    "                       alpha=0.7)\n",
    "                \n",
    "                ax.errorbar(positions, accuracies,\n",
    "                           yerr=errors, fmt='none', color='black', capsize=3)\n",
    "            \n",
    "            bar_idx += 1\n",
    "\n",
    "    # Customize each subplot\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(f'Task: {task}')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f'{fps} FPS' for fps in fps_values])\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Add horizontal line at 0.5 for random chance\n",
    "    ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.3, label='Random Chance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print best combinations\n",
    "print(\"\\nBest performing combinations for each task:\")\n",
    "for task in tasks:\n",
    "    task_data = task_model_fps_results[task_model_fps_results['task'] == task]\n",
    "    best_row = task_data.loc[task_data['accuracy'].idxmax()]\n",
    "    print(f\"\\n{task}:\")\n",
    "    print(f\"  VLM: {best_row['vlm']}\")\n",
    "    print(f\"  FPS: {best_row['fps']}\")\n",
    "    print(f\"  Method: {best_row['method']}\")\n",
    "    print(f\"  Accuracy: {best_row['accuracy']:.3f} Â± {best_row['std_err']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe for vote analysis\n",
    "vote_analysis = pd.DataFrame()\n",
    "\n",
    "# Extract performance lists and analyze different numbers of votes\n",
    "for idx, row in df.iterrows():\n",
    "    perf = row['votes_float']\n",
    "    if isinstance(perf, list):\n",
    "        # For each number of votes (1, 3, 5)\n",
    "        for n_votes in [1, 3, 5]:\n",
    "            # Take first n_votes if available\n",
    "            votes = perf[:n_votes]\n",
    "            if len(votes) >= n_votes:\n",
    "                # Calculate mean performance with this many votes\n",
    "                mean_perf = np.mean(votes)\n",
    "                vote_analysis = pd.concat([vote_analysis, pd.DataFrame({\n",
    "                    'vlm': [row['vlm']],\n",
    "                    'task': [row['task']],\n",
    "                    'method': [row['method']],  # Added method\n",
    "                    'n_votes': [n_votes],\n",
    "                    'mean_performance': [mean_perf],\n",
    "                    'true_success': [1 if row['success_flag'] == 'success' else 0]\n",
    "                })])\n",
    "vote_analysis['accuracy_of_mean'] = vote_analysis['true_success'] == (vote_analysis['mean_performance'] > 0.5)\n",
    "\n",
    "# Calculate accuracy for each VLM, method, and number of votes\n",
    "vote_results = vote_analysis.groupby(['vlm', 'method', 'n_votes']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'accuracy': x['accuracy_of_mean'].mean(),\n",
    "        'std_err': x['accuracy_of_mean'].std() / np.sqrt(len(x))\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "# Create separate plots for each method\n",
    "methods = vote_results['method'].unique()\n",
    "fig, axes = plt.subplots(len(methods), 1, figsize=(12, 5*len(methods)))\n",
    "if len(methods) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for method_idx, (method, ax) in enumerate(zip(methods, axes)):\n",
    "    # Filter data for this method\n",
    "    method_data = vote_results[vote_results['method'] == method]\n",
    "    \n",
    "    # Plot each VLM for this method\n",
    "    for vlm in method_data['vlm'].unique():\n",
    "        vlm_data = method_data[method_data['vlm'] == vlm]\n",
    "        if not vlm_data.empty:\n",
    "            ax.errorbar(vlm_data['n_votes'], \n",
    "                       vlm_data['accuracy'], \n",
    "                       yerr=vlm_data['std_err'],\n",
    "                       label=vlm, \n",
    "                       marker='o', \n",
    "                       capsize=5)\n",
    "    \n",
    "    # Customize subplot\n",
    "    ax.set_xlabel('Number of Votes')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(f'Accuracy vs Number of Votes by Model ({method})')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.3, label='Random Chance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For task_results\n",
    "task_results = df.groupby(['task', 'method']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'accuracy': x['accuracy_of_mean'].mean(),\n",
    "        'std_err': x['accuracy_of_mean'].std() / np.sqrt(len(x))\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# For fps_results and related dataframes\n",
    "fps_results = df.groupby(['vlm', 'fps', 'method']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'accuracy': x['accuracy_of_mean'].mean(),\n",
    "        'std_err': x['accuracy_of_mean'].std() / np.sqrt(len(x))\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "fps_model_mean_accs = df.groupby(['vlm', 'fps', 'method']).apply(\n",
    "    lambda x: x['accuracy_of_mean'].mean()\n",
    ").reset_index(name='accuracy')\n",
    "\n",
    "fps_model_median_accs = df.groupby(['vlm', 'fps', 'method']).apply(\n",
    "    lambda x: x['accuracy_of_median'].mean()\n",
    ").reset_index(name='accuracy')\n",
    "\n",
    "# For task_model_results\n",
    "task_model_results = df.groupby(['task', 'vlm', 'method']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'accuracy': x['accuracy_of_mean'].mean(),\n",
    "        'std_err': x['accuracy_of_mean'].std() / np.sqrt(len(x))\n",
    "    })\n",
    ").reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique methods\n",
    "methods = fps_model_mean_accs['method'].unique()\n",
    "\n",
    "# Create subplot for each method\n",
    "fig, axes = plt.subplots(len(methods), 1, figsize=(10, 6*len(methods)))\n",
    "if len(methods) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Plot for each method\n",
    "for method_idx, (method, ax) in enumerate(zip(methods, axes)):\n",
    "    # Filter data for this method\n",
    "    method_mean_data = fps_model_mean_accs[fps_model_mean_accs['method'] == method]\n",
    "    method_median_data = fps_model_median_accs[fps_model_median_accs['method'] == method]\n",
    "    \n",
    "    # Plot for each VLM within this method\n",
    "    for i, vlm in enumerate(method_mean_data['vlm'].unique()):\n",
    "        # Plot mean accuracy\n",
    "        vlm_data = method_mean_data[method_mean_data['vlm'] == vlm]\n",
    "        ax.plot(vlm_data['fps'], vlm_data['accuracy'], \n",
    "                marker='o', label=f'{vlm} (mean)', linestyle='-')\n",
    "        \n",
    "        # Plot median accuracy\n",
    "        vlm_data = method_median_data[method_median_data['vlm'] == vlm]\n",
    "        ax.plot(vlm_data['fps'], vlm_data['accuracy'], \n",
    "                marker='s', label=f'{vlm} (median)', linestyle='--')\n",
    "    \n",
    "    # Customize subplot\n",
    "    ax.set_xlabel('FPS')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(f'Performance Accuracy vs FPS ({method})')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.3, label='Random Chance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per task performance\n",
    "# Group by task and calculate mean accuracy\n",
    "task_results = df.groupby(['task']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'accuracy': x['accuracy_of_mean'].mean(),\n",
    "        'std_err': x['accuracy_of_mean'].std() / np.sqrt(len(x))\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(task_results['task'], task_results['accuracy'])\n",
    "plt.errorbar(task_results['task'], task_results['accuracy'], \n",
    "             yerr=task_results['std_err'], fmt='none', color='black', capsize=5)\n",
    "\n",
    "# add horizontal line at 0.5\n",
    "plt.axhline(y=0.5, color='red', linestyle='--', label='Random Chance')\n",
    "# Customize plot\n",
    "plt.xlabel('Task')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Performance Accuracy by Task')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTask-wise Performance:\")\n",
    "print(task_results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by task and VLM to calculate mean accuracy\n",
    "task_model_results = df.groupby(['task', 'vlm']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'accuracy': x['accuracy_of_mean'].mean(),\n",
    "        'std_err': x['accuracy_of_mean'].std() / np.sqrt(len(x))\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Get unique tasks and VLMs\n",
    "# Get unique tasks and VLMs\n",
    "tasks = task_model_results['task'].unique()\n",
    "vlms = task_model_results['vlm'].unique()\n",
    "x = np.arange(len(tasks))\n",
    "width = 0.8 / len(vlms)  # Width of bars with spacing\n",
    "\n",
    "# Plot bars for each VLM\n",
    "for i, vlm in enumerate(vlms):\n",
    "    vlm_data = task_model_results[task_model_results['vlm'] == vlm]\n",
    "    # Calculate offset for this VLM's bars\n",
    "    offset = (i - len(vlms)/2 + 0.5) * width\n",
    "    # Match tasks with the current VLM's data\n",
    "    accuracies = []\n",
    "    positions = []\n",
    "    errors = []\n",
    "    \n",
    "    for task_idx, task in enumerate(tasks):\n",
    "        task_data = vlm_data[vlm_data['task'] == task]\n",
    "        if not task_data.empty:\n",
    "            positions.append(task_idx + offset)\n",
    "            accuracies.append(task_data['accuracy'].iloc[0])\n",
    "            errors.append(task_data['std_err'].iloc[0])\n",
    "    \n",
    "    plt.bar(positions, accuracies, width, label=vlm)\n",
    "    plt.errorbar(positions, accuracies,\n",
    "                yerr=errors, fmt='none', color='black', capsize=3)\n",
    "\n",
    "# Customize plot\n",
    "plt.xlabel('Task')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Performance Accuracy by Task and Model')\n",
    "plt.xticks(x, tasks, rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by task, VLM, and FPS to calculate mean accuracy\n",
    "task_model_fps_results = df.groupby(['task', 'vlm', 'fps']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'accuracy': x['accuracy_of_mean'].mean(),\n",
    "        'std_err': x['accuracy_of_mean'].std() / np.sqrt(len(x))\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Get unique values\n",
    "tasks = task_model_fps_results['task'].unique()\n",
    "vlms = task_model_fps_results['vlm'].unique()\n",
    "fps_values = sorted(task_model_fps_results['fps'].unique())\n",
    "\n",
    "# Create subplot for each task\n",
    "fig, axes = plt.subplots(len(tasks), 1, figsize=(12, 5*len(tasks)))\n",
    "if len(tasks) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for task_idx, (task, ax) in enumerate(zip(tasks, axes)):\n",
    "    task_data = task_model_fps_results[task_model_fps_results['task'] == task]\n",
    "    x = np.arange(len(fps_values))\n",
    "    width = 0.8 / len(vlms)  # Width of bars with spacing\n",
    "\n",
    "    # Plot bars for each VLM\n",
    "    for i, vlm in enumerate(vlms):\n",
    "        # Calculate offset for this VLM's bars\n",
    "        offset = (i - len(vlms)/2 + 0.5) * width\n",
    "        \n",
    "        accuracies = []\n",
    "        errors = []\n",
    "        positions = []\n",
    "        \n",
    "        for j, fps in enumerate(fps_values):\n",
    "            vlm_data = task_data[(task_data['vlm'] == vlm) & (task_data['fps'] == fps)]\n",
    "            if not vlm_data.empty:\n",
    "                positions.append(j + offset)\n",
    "                accuracies.append(vlm_data['accuracy'].iloc[0])\n",
    "                errors.append(vlm_data['std_err'].iloc[0])\n",
    "        \n",
    "        ax.bar(positions, accuracies, width, \n",
    "               label=vlm,\n",
    "               alpha=0.7)\n",
    "        \n",
    "        ax.errorbar(positions, accuracies,\n",
    "                   yerr=errors, fmt='none', color='black', capsize=3)\n",
    "\n",
    "    # Customize each subplot\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(f'Task: {task}')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f'{fps} FPS' for fps in fps_values])\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add horizontal line at 0.5 for random chance\n",
    "    ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.3, label='Random Chance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print best combinations\n",
    "print(\"\\nBest performing combinations for each task:\")\n",
    "for task in tasks:\n",
    "    task_data = task_model_fps_results[task_model_fps_results['task'] == task]\n",
    "    best_row = task_data.loc[task_data['accuracy'].idxmax()]\n",
    "    print(f\"\\n{task}:\")\n",
    "    print(f\"  VLM: {best_row['vlm']}\")\n",
    "    print(f\"  FPS: {best_row['fps']}\")\n",
    "    print(f\"  Accuracy: {best_row['accuracy']:.3f} Â± {best_row['std_err']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall method performance\n",
    "method_results = df.groupby(['method']).agg({\n",
    "    'accuracy_of_mean': ['mean', lambda x: x.std() / np.sqrt(len(x))]\n",
    "}).reset_index()\n",
    "method_results.columns = ['method', 'accuracy', 'std_err']\n",
    "\n",
    "# Method performance by model\n",
    "model_method_results = df.groupby(['vlm', 'method']).agg({\n",
    "    'accuracy_of_mean': ['mean', lambda x: x.std() / np.sqrt(len(x))]\n",
    "}).reset_index()\n",
    "model_method_results.columns = ['vlm', 'method', 'accuracy', 'std_err']\n",
    "\n",
    "# Create two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Overall method performance\n",
    "ax1.bar(method_results['method'], method_results['accuracy'])\n",
    "ax1.errorbar(method_results['method'], method_results['accuracy'], \n",
    "             yerr=method_results['std_err'], fmt='none', color='black', capsize=5)\n",
    "ax1.set_title('Overall Method Performance')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=0.5, color='red', linestyle='--', alpha=0.3, label='Random Chance')\n",
    "\n",
    "# Plot 2: Method performance by model\n",
    "vlms = model_method_results['vlm'].unique()\n",
    "methods = model_method_results['method'].unique()\n",
    "x = np.arange(len(vlms))\n",
    "width = 0.35\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    method_data = model_method_results[model_method_results['method'] == method]\n",
    "    offset = (i - len(methods)/2 + 0.5) * width\n",
    "    \n",
    "    # Ensure data aligns with x-axis positions\n",
    "    accuracies = []\n",
    "    errors = []\n",
    "    for vlm in vlms:\n",
    "        vlm_data = method_data[method_data['vlm'] == vlm]\n",
    "        if not vlm_data.empty:\n",
    "            accuracies.append(vlm_data['accuracy'].iloc[0])\n",
    "            errors.append(vlm_data['std_err'].iloc[0])\n",
    "        else:\n",
    "            accuracies.append(0)\n",
    "            errors.append(0)\n",
    "    \n",
    "    ax2.bar(x + offset, accuracies, width, label=method)\n",
    "    ax2.errorbar(x + offset, accuracies, \n",
    "                yerr=errors, fmt='none', color='black', capsize=3)\n",
    "\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Method Performance by Model')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(vlms, rotation=45)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0.5, color='red', linestyle='--', alpha=0.3, label='Random Chance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print numerical results\n",
    "print(\"\\nOverall Method Performance:\")\n",
    "print(method_results.to_string(index=False))\n",
    "print(\"\\nMethod Performance by Model:\")\n",
    "print(model_method_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
