{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note that 0 FPS means just first and last frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potentially combine paths!\n",
    "paths = [\n",
    "    \"/tmp/eval_results_2024-12-26_19-26-57.csv\",\n",
    "    \"/tmp/eval_results_2024-12-26_19-58-11.csv\",\n",
    "    \"/tmp/eval_results_2024-12-26_20-27-29.csv\",\n",
    "    \"/tmp/eval_results_2024-12-26_20-42-07.csv\",\n",
    "    \"/tmp/eval_results_gpt-4o-mini_2024-12-26_23-38-09.csv\",\n",
    "    \"/tmp/eval_results_gpt-4o_2024-12-27_01-42-46.csv\",\n",
    "    \"/tmp/eval_results_gemini-2.0-flash-exp_2024-12-27_02-14-20.csv\",\n",
    "    \"/tmp/eval_results_gemini-1.5-pro_2024-12-27_03-01-14.csv\",\n",
    "    \"/tmp/eval_results_gpt-4o_2024-12-27_21-18-54.csv\",\n",
    "    \"/tmp/eval_results_gpt-4o-mini_2024-12-27_21-24-10.csv\",\n",
    "    \"/tmp/eval_results_gemini-1.5-pro_2024-12-27_21-31-39.csv\",\n",
    "    \"/tmp/eval_results_gemini-2.0-flash-exp_2024-12-27_21-36-22.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv(path) for path in paths])\n",
    "df['label'] = df['success_flag'].apply(lambda x: 1 if x== 'success' else 0)\n",
    "df['accuracy_of_mean'] = df['label'] == (df['mean_performance'] > 0.5)\n",
    "df['accuracy_of_median'] = df['label'] == (df['median_performance'] > 0.5)\n",
    "\n",
    "df['vote_str'] = df['performance']\n",
    "df['votes_float'] = df['performance'].apply(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['accuracy_of_mean'].mean(), df['accuracy_of_median'].mean(), df['votes_float'].apply(lambda x: len(x)).mean())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for computing means and std errors\n",
    "def mean_stderr(data):\n",
    "    mean = np.mean(data)\n",
    "    stderr = np.std(data) / np.sqrt(len(data))\n",
    "    return mean, stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance of mean vs median for each model \n",
    "# Group by VLM and compute accuracy for both mean and median methods\n",
    "grouped_acc = df.groupby('vlm').agg({\n",
    "    'accuracy_of_mean': ['mean', 'std'],\n",
    "    'accuracy_of_median': ['mean', 'std']\n",
    "}).round(3)\n",
    "\n",
    "# Create bar plot comparing mean vs median performance\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(len(grouped_acc.index))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, grouped_acc['accuracy_of_mean']['mean'], width, \n",
    "        label='Mean threshold', \n",
    "        yerr=grouped_acc['accuracy_of_mean']['std'],\n",
    "        capsize=5)\n",
    "plt.bar(x + width/2, grouped_acc['accuracy_of_median']['mean'], width,\n",
    "        label='Median threshold',\n",
    "        yerr=grouped_acc['accuracy_of_median']['std'], \n",
    "        capsize=5)\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Performance Metric Comparison by Model')\n",
    "plt.xticks(x, grouped_acc.index, rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, what about as we change the number of votes? 1 vs 3 vs 5? get the votes from the 'performance' column. some are nan / not full length, just reuse\n",
    "\n",
    "# Create a new dataframe for vote analysis\n",
    "vote_analysis = pd.DataFrame()\n",
    "\n",
    "# Extract performance lists and analyze different numbers of votes\n",
    "for idx, row in df.iterrows():\n",
    "    perf = row['votes_float']\n",
    "    if isinstance(perf, list):\n",
    "        # For each number of votes (1, 3, 5)\n",
    "        for n_votes in [1, 3, 5]:\n",
    "            # Take first n_votes if available\n",
    "            votes = perf[:n_votes]\n",
    "            if len(votes) >= n_votes:\n",
    "                # Calculate mean performance with this many votes\n",
    "                mean_perf = np.mean(votes)\n",
    "                vote_analysis = pd.concat([vote_analysis, pd.DataFrame({\n",
    "                    'vlm': [row['vlm']],\n",
    "                    'task': [row['task']],\n",
    "                    'n_votes': [n_votes],\n",
    "                    'mean_performance': [mean_perf],\n",
    "                    'true_success': [1 if row['success_flag'] == 'success' else 0]\n",
    "                })])\n",
    "vote_analysis['accuracy_of_mean'] = vote_analysis['true_success'] == (vote_analysis['mean_performance'] > 0.5)\n",
    "\n",
    "# Calculate accuracy for each VLM and number of votes\n",
    "vote_results = vote_analysis.groupby(['vlm', 'n_votes']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'accuracy': x['accuracy_of_mean'].mean(),\n",
    "        'std_err': x['accuracy_of_mean'].std() / np.sqrt(len(x))\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "for vlm in vote_results['vlm'].unique():\n",
    "    vlm_data = vote_results[vote_results['vlm'] == vlm]\n",
    "    plt.errorbar(vlm_data['n_votes'], vlm_data['accuracy'], \n",
    "                yerr=vlm_data['std_err'],\n",
    "                label=vlm, marker='o', capsize=5)\n",
    "\n",
    "plt.xlabel('Number of Votes')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Number of Votes by Model')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate accuracy for each VLM and FPS\n",
    "fps_results = df.groupby(['vlm', 'fps']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'accuracy': x['accuracy_of_mean'].mean(),\n",
    "        'std_err': x['accuracy_of_mean'].std() / np.sqrt(len(x))\n",
    "    })\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps_model_mean_accs = df.groupby(['vlm', 'fps']).apply(\n",
    "    lambda x: x['accuracy_of_mean'].mean()\n",
    ").reset_index(name='accuracy')\n",
    "\n",
    "fps_model_median_accs = df.groupby(['vlm', 'fps']).apply(\n",
    "    lambda x: x['accuracy_of_median'].mean()\n",
    ").reset_index(name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot both mean and median accuracy on same plot\n",
    "for i, vlm in enumerate(fps_model_mean_accs['vlm'].unique()):\n",
    "    # Plot mean accuracy\n",
    "    vlm_data = fps_model_mean_accs[fps_model_mean_accs['vlm'] == vlm]\n",
    "    plt.plot(vlm_data['fps'], vlm_data['accuracy'], marker='o', label=f'{vlm} (mean)', linestyle='-')\n",
    "    \n",
    "    # Plot median accuracy\n",
    "    vlm_data = fps_model_median_accs[fps_model_median_accs['vlm'] == vlm]\n",
    "    plt.plot(vlm_data['fps'] , vlm_data['accuracy'], marker='s', label=f'{vlm} (median)', linestyle='--')\n",
    "\n",
    "plt.xlabel('FPS')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Performance Accuracy vs FPS')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per task performance\n",
    "# Group by task and calculate mean accuracy\n",
    "task_results = df.groupby(['task']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'accuracy': x['accuracy_of_mean'].mean(),\n",
    "        'std_err': x['accuracy_of_mean'].std() / np.sqrt(len(x))\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(task_results['task'], task_results['accuracy'])\n",
    "plt.errorbar(task_results['task'], task_results['accuracy'], \n",
    "             yerr=task_results['std_err'], fmt='none', color='black', capsize=5)\n",
    "\n",
    "# add horizontal line at 0.5\n",
    "plt.axhline(y=0.5, color='red', linestyle='--', label='Random Chance')\n",
    "# Customize plot\n",
    "plt.xlabel('Task')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Performance Accuracy by Task')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTask-wise Performance:\")\n",
    "print(task_results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by task and VLM to calculate mean accuracy\n",
    "task_model_results = df.groupby(['task', 'vlm']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'accuracy': x['accuracy_of_mean'].mean(),\n",
    "        'std_err': x['accuracy_of_mean'].std() / np.sqrt(len(x))\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Get unique tasks and VLMs\n",
    "# Get unique tasks and VLMs\n",
    "tasks = task_model_results['task'].unique()\n",
    "vlms = task_model_results['vlm'].unique()\n",
    "x = np.arange(len(tasks))\n",
    "width = 0.8 / len(vlms)  # Width of bars with spacing\n",
    "\n",
    "# Plot bars for each VLM\n",
    "for i, vlm in enumerate(vlms):\n",
    "    vlm_data = task_model_results[task_model_results['vlm'] == vlm]\n",
    "    # Calculate offset for this VLM's bars\n",
    "    offset = (i - len(vlms)/2 + 0.5) * width\n",
    "    # Match tasks with the current VLM's data\n",
    "    accuracies = []\n",
    "    positions = []\n",
    "    errors = []\n",
    "    \n",
    "    for task_idx, task in enumerate(tasks):\n",
    "        task_data = vlm_data[vlm_data['task'] == task]\n",
    "        if not task_data.empty:\n",
    "            positions.append(task_idx + offset)\n",
    "            accuracies.append(task_data['accuracy'].iloc[0])\n",
    "            errors.append(task_data['std_err'].iloc[0])\n",
    "    \n",
    "    plt.bar(positions, accuracies, width, label=vlm)\n",
    "    plt.errorbar(positions, accuracies,\n",
    "                yerr=errors, fmt='none', color='black', capsize=3)\n",
    "\n",
    "# Customize plot\n",
    "plt.xlabel('Task')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Performance Accuracy by Task and Model')\n",
    "plt.xticks(x, tasks, rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by task, VLM, and FPS to calculate mean accuracy\n",
    "task_model_fps_results = df.groupby(['task', 'vlm', 'fps']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'accuracy': x['accuracy_of_mean'].mean(),\n",
    "        'std_err': x['accuracy_of_mean'].std() / np.sqrt(len(x))\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Get unique values\n",
    "tasks = task_model_fps_results['task'].unique()\n",
    "vlms = task_model_fps_results['vlm'].unique()\n",
    "fps_values = sorted(task_model_fps_results['fps'].unique())\n",
    "\n",
    "# Create subplot for each task\n",
    "fig, axes = plt.subplots(len(tasks), 1, figsize=(12, 5*len(tasks)))\n",
    "if len(tasks) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for task_idx, (task, ax) in enumerate(zip(tasks, axes)):\n",
    "    task_data = task_model_fps_results[task_model_fps_results['task'] == task]\n",
    "    x = np.arange(len(fps_values))\n",
    "    width = 0.8 / len(vlms)  # Width of bars with spacing\n",
    "\n",
    "    # Plot bars for each VLM\n",
    "    for i, vlm in enumerate(vlms):\n",
    "        # Calculate offset for this VLM's bars\n",
    "        offset = (i - len(vlms)/2 + 0.5) * width\n",
    "        \n",
    "        accuracies = []\n",
    "        errors = []\n",
    "        positions = []\n",
    "        \n",
    "        for j, fps in enumerate(fps_values):\n",
    "            vlm_data = task_data[(task_data['vlm'] == vlm) & (task_data['fps'] == fps)]\n",
    "            if not vlm_data.empty:\n",
    "                positions.append(j + offset)\n",
    "                accuracies.append(vlm_data['accuracy'].iloc[0])\n",
    "                errors.append(vlm_data['std_err'].iloc[0])\n",
    "        \n",
    "        ax.bar(positions, accuracies, width, \n",
    "               label=vlm,\n",
    "               alpha=0.7)\n",
    "        \n",
    "        ax.errorbar(positions, accuracies,\n",
    "                   yerr=errors, fmt='none', color='black', capsize=3)\n",
    "\n",
    "    # Customize each subplot\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(f'Task: {task}')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f'{fps} FPS' for fps in fps_values])\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add horizontal line at 0.5 for random chance\n",
    "    ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.3, label='Random Chance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print best combinations\n",
    "print(\"\\nBest performing combinations for each task:\")\n",
    "for task in tasks:\n",
    "    task_data = task_model_fps_results[task_model_fps_results['task'] == task]\n",
    "    best_row = task_data.loc[task_data['accuracy'].idxmax()]\n",
    "    print(f\"\\n{task}:\")\n",
    "    print(f\"  VLM: {best_row['vlm']}\")\n",
    "    print(f\"  FPS: {best_row['fps']}\")\n",
    "    print(f\"  Accuracy: {best_row['accuracy']:.3f} ± {best_row['std_err']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
