{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note that 0 FPS means just first and last frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potentially combine paths!\n",
    "paths = [\n",
    "    \"/workspaces/ares/data/eval_dump/eval_results_gpt-4o-mini_2025-01-06_21-54-15_video.csv\",\n",
    "    \"/workspaces/ares/data/eval_dump/eval_results_gpt-4o_2025-01-06_22-00-33_video.csv\",\n",
    "    \"/workspaces/ares/data/eval_dump/eval_results_gemini-1.5-pro_2025-01-06_22-04-12_video.csv\",\n",
    "    \"/workspaces/ares/data/eval_dump/eval_results_gemini-1.5-pro_2025-01-06_23-07-26_frame_descriptions.csv\",\n",
    "    \"/workspaces/ares/data/eval_dump/eval_results_gpt-4o_2025-01-07_00-33-35_frame_descriptions.csv\",\n",
    "    \"/workspaces/ares/data/eval_dump/eval_results_gpt-4o-mini_2025-01-07_01-11-20_frame_descriptions.csv\"\n",
    "\n",
    "    # \"/workspaces/ares/data/eval_dump/eval_results_gpt-4o-mini_2025-02-13_16-59-19_video.csv\",\n",
    "    # \"/workspaces/ares/data/eval_dump/eval_results_gpt-4o_2025-02-13_17-22-06_video.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "# collect all the DFs and assign labels according to success_flag\n",
    "# calculate accuracy of mean and median\n",
    "# calculate vote performance\n",
    "df = pd.concat([pd.read_csv(path) for path in paths])\n",
    "df['label'] = df['success_flag'] if not isinstance(df.success_flag.iloc[0], str) else df.success_flag.apply(lambda x: float(x == 'success'))\n",
    "# calculate if each prediction is correct\n",
    "df['accuracy_of_mean'] = df['label'] == (df['mean_performance'] > 0.5)\n",
    "df['accuracy_of_median'] = df['label'] == (df['median_performance'] > 0.5)\n",
    "# transform performance into a list of floats for each row\n",
    "df['vote_str'] = df['performance']\n",
    "\n",
    "def converter(performance):\n",
    "    performance = performance.replace('nan', 'null') # convert for json loads\n",
    "    return json.loads(performance)\n",
    "\n",
    "df['votes_float'] = df['performance'].apply(converter)\n",
    "\n",
    "for i in range(5):\n",
    "    df[f'mean_performance_n={i+1}'] = df['votes_float'].apply(lambda x: np.mean(x[:i+1]))\n",
    "    df[f'median_performance_n={i+1}'] = df['votes_float'].apply(lambda x: np.median(x[:i+1]))\n",
    "    df[f'accuracy_of_mean_n={i+1}'] = df['label'] == (df[f'mean_performance_n={i+1}'] > 0.5)\n",
    "    df[f'accuracy_of_median_n={i+1}'] = df['label'] == (df[f'median_performance_n={i+1}'] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in df.groupby(['vlm', 'method', 'fps']):\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{df['accuracy_of_mean'].mean():.3f}, {df['accuracy_of_median'].mean():.3f}, {df['votes_float'].apply(lambda x: len(x)).mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(f\"{df[f'accuracy_of_mean_n={i+1}'].mean():.3f}, {df[f'accuracy_of_median_n={i+1}'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for computing means and std errors\n",
    "def mean_stderr(data):\n",
    "    mean = np.mean(data)\n",
    "    stderr = np.std(data) / np.sqrt(len(data))\n",
    "    return mean, stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "def _create_line_plot(ax, df, primary_axis, other_axes, unique_values, score_title):\n",
    "    \"\"\"Helper function to create a line plot on given axis\"\"\"\n",
    "    other_combinations = list(product(*(unique_values[ax_name] for ax_name in other_axes)))\n",
    "    \n",
    "    for combo in other_combinations:\n",
    "        filtered_df = df.copy()\n",
    "        for ax_name, value in zip(other_axes, combo):\n",
    "            filtered_df = filtered_df[filtered_df[ax_name] == value]\n",
    "        \n",
    "        groups = filtered_df.groupby(primary_axis)\n",
    "        xs = []\n",
    "        ys = []\n",
    "        \n",
    "        for name, group in groups:\n",
    "            xs.append(name)\n",
    "            ys.append(group[score_title].mean())\n",
    "        \n",
    "        label = \", \".join(f\"{ax}={val}\" for ax, val in zip(other_axes, combo))\n",
    "        ax.plot(xs, ys, marker='o', label=label, linewidth=2, markersize=8)\n",
    "\n",
    "def _create_bar_plot(ax, df, primary_axis, other_axes, unique_values, score_title):\n",
    "    \"\"\"Helper function to create a bar plot on given axis\"\"\"\n",
    "    other_combinations = list(product(*(unique_values[ax_name] for ax_name in other_axes)))\n",
    "    primary_values = unique_values[primary_axis]\n",
    "    \n",
    "    x = np.arange(len(primary_values))\n",
    "    total_bars = len(other_combinations)\n",
    "    width = 0.8 / total_bars\n",
    "    \n",
    "    for combo_idx, combo in enumerate(other_combinations):\n",
    "        filtered_df = df.copy()\n",
    "        for ax_name, value in zip(other_axes, combo):\n",
    "            filtered_df = filtered_df[filtered_df[ax_name] == value]\n",
    "        \n",
    "        groups = filtered_df.groupby(primary_axis)\n",
    "        values = {}\n",
    "        \n",
    "        for name, group in groups:\n",
    "            values[name] = group[score_title].mean()\n",
    "        \n",
    "        heights = [values.get(val, 0) for val in primary_values]\n",
    "        offset = (combo_idx - total_bars/2 + 0.5) * width\n",
    "        \n",
    "        label = \", \".join(f\"{ax}={val}\" for ax, val in zip(other_axes, combo))\n",
    "        ax.bar(x + offset, heights, width, label=label, alpha=0.8)\n",
    "    \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(primary_values, rotation=45, ha='right')\n",
    "\n",
    "def _style_axis(ax, primary_axis, metric_name):\n",
    "    \"\"\"Helper function to apply consistent styling to an axis\"\"\"\n",
    "    ax.set_title(f'{metric_name} Performance by {primary_axis}', fontsize=12, pad=15)\n",
    "    ax.set_xlabel(primary_axis, fontsize=10)\n",
    "    ax.set_ylabel('Accuracy', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.3, label='Random Chance')\n",
    "    ax.set_ylim(0.4, 1.0)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=f'{metric_name} Performance', title_fontsize=10)\n",
    "\n",
    "def plot_along_axes(df, axis_names: list[str], score_title_list: list[str], chart_type='line'):\n",
    "    \"\"\"Main plotting function that creates individual metric plots and optionally a combined plot\"\"\"\n",
    "    # Create figure with appropriate number of subplots\n",
    "    n_metrics = len(score_title_list)\n",
    "    n_plots = n_metrics + 1 if n_metrics > 1 else n_metrics  # Only add combined plot if multiple metrics\n",
    "    fig, axes = plt.subplots(1, n_plots, figsize=(12 * n_plots, 6))\n",
    "    \n",
    "    # Ensure axes is always a numpy array\n",
    "    if not isinstance(axes, np.ndarray):\n",
    "        axes = np.array([axes])\n",
    "    axes = np.atleast_1d(axes)\n",
    "    \n",
    "    # Get unique values for each axis\n",
    "    unique_values = {name: sorted(df[name].unique()) for name in axis_names}\n",
    "    primary_axis = axis_names[0]\n",
    "    other_axes = axis_names[1:]\n",
    "    \n",
    "    # Create individual metric plots\n",
    "    for ax_idx, (ax, score_title) in enumerate(zip(axes[:n_metrics], score_title_list)):\n",
    "        if chart_type == 'line':\n",
    "            _create_line_plot(ax, df, primary_axis, other_axes, unique_values, score_title)\n",
    "        elif chart_type == 'bar':\n",
    "            _create_bar_plot(ax, df, primary_axis, other_axes, unique_values, score_title)\n",
    "        \n",
    "        metric_name = score_title.replace('accuracy_of_', '').replace(\"_\", \" \").title()\n",
    "        _style_axis(ax, primary_axis, metric_name)\n",
    "    \n",
    "    # Create combined plot only if there are multiple metrics\n",
    "    if n_metrics > 1:\n",
    "        if chart_type == 'line':\n",
    "            other_combinations = list(product(*(unique_values[ax_name] for ax_name in other_axes)))\n",
    "            \n",
    "            for combo in other_combinations:\n",
    "                filtered_df = df.copy()\n",
    "                for ax_name, value in zip(other_axes, combo):\n",
    "                    filtered_df = filtered_df[filtered_df[ax_name] == value]\n",
    "                \n",
    "                groups = filtered_df.groupby(primary_axis)\n",
    "                xs = []\n",
    "                ys = defaultdict(list)\n",
    "                \n",
    "                for name, group in groups:\n",
    "                    xs.append(name)\n",
    "                    for score_title in score_title_list:\n",
    "                        ys[score_title].append(group[score_title].mean())\n",
    "                \n",
    "                combo_label = \", \".join(f\"{ax}={val}\" for ax, val in zip(other_axes, combo))\n",
    "                for score_title in score_title_list:\n",
    "                    metric_name = score_title.replace('accuracy_of_', '').replace(\"_\", \" \").title()\n",
    "                    axes[-1].plot(xs, ys[score_title], marker='o', \n",
    "                                label=f'{metric_name} - {combo_label}', \n",
    "                                linewidth=2, markersize=8)\n",
    "        elif chart_type == 'bar':\n",
    "            other_combinations = list(product(*(unique_values[ax_name] for ax_name in other_axes)))\n",
    "            primary_values = unique_values[primary_axis]\n",
    "            \n",
    "            x = np.arange(len(primary_values))\n",
    "            total_bars = len(other_combinations) * len(score_title_list)\n",
    "            width = 0.8 / total_bars\n",
    "            \n",
    "            bar_idx = 0\n",
    "            for combo in other_combinations:\n",
    "                filtered_df = df.copy()\n",
    "                for ax_name, value in zip(other_axes, combo):\n",
    "                    filtered_df = filtered_df[filtered_df[ax_name] == value]\n",
    "                \n",
    "                groups = filtered_df.groupby(primary_axis)\n",
    "                values = defaultdict(dict)\n",
    "                \n",
    "                for name, group in groups:\n",
    "                    for score_title in score_title_list:\n",
    "                        values[score_title][name] = group[score_title].mean()\n",
    "                \n",
    "                combo_label = \", \".join(f\"{ax}={val}\" for ax, val in zip(other_axes, combo))\n",
    "                \n",
    "                for score_title in score_title_list:\n",
    "                    heights = [values[score_title].get(val, 0) for val in primary_values]\n",
    "                    offset = (bar_idx - total_bars/2 + 0.5) * width\n",
    "                    \n",
    "                    metric_name = score_title.replace('accuracy_of_', '').replace(\"_\", \" \").title()\n",
    "                    axes[-1].bar(x + offset, heights, width, \n",
    "                               label=f'{metric_name} - {combo_label}', alpha=0.8)\n",
    "                    bar_idx += 1\n",
    "            \n",
    "            axes[-1].set_xticks(x)\n",
    "            axes[-1].set_xticklabels(primary_values, rotation=45, ha='right')\n",
    "\n",
    "\n",
    "        _style_axis(axes[-1], primary_axis, \"Combined\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_along_axes(df, \n",
    "              axis_names=['fps', 'vlm'], \n",
    "              score_title_list=['accuracy_of_mean_n=5', 'accuracy_of_median_n=5'], \n",
    "              chart_type='line')\n",
    "\n",
    "plot_along_axes(df, \n",
    "              axis_names=['fps', 'vlm'], \n",
    "              score_title_list=['accuracy_of_mean_n=5', 'accuracy_of_median_n=5'], \n",
    "              chart_type='bar')\n",
    "\n",
    "plot_along_axes(df, \n",
    "              axis_names=['method'], \n",
    "              score_title_list=['accuracy_of_mean_n=5', 'accuracy_of_median_n=5'], \n",
    "              chart_type='bar')\n",
    "\n",
    "plot_along_axes(df, \n",
    "              axis_names=['task'], \n",
    "              score_title_list=['accuracy_of_mean_n=5', 'accuracy_of_median_n=5'], \n",
    "              chart_type='bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with n as a column\n",
    "plot_df = df.copy()\n",
    "plot_df = pd.melt(plot_df, \n",
    "                  id_vars=['fps', 'vlm', 'method', 'task'], \n",
    "                  value_vars=[f'accuracy_of_mean_n={i+1}' for i in range(5)],\n",
    "                  var_name='n_votes',\n",
    "                  value_name='accuracy')\n",
    "\n",
    "# Extract n value from the column name\n",
    "plot_df['n'] = plot_df['n_votes'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# Now we can plot with n as our primary axis\n",
    "plot_along_axes(plot_df, \n",
    "              axis_names=['n', 'vlm'], \n",
    "              score_title_list=['accuracy'], \n",
    "              chart_type='line')\n",
    "\n",
    "# Or with different groupings\n",
    "plot_along_axes(plot_df, \n",
    "              axis_names=['n', 'fps'], \n",
    "              score_title_list=['accuracy'], \n",
    "              chart_type='line')\n",
    "\n",
    "# Or both as bars\n",
    "plot_along_axes(plot_df, \n",
    "              axis_names=['n', 'vlm'], \n",
    "              score_title_list=['accuracy'], \n",
    "              chart_type='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by task, VLM, FPS, and method to calculate mean accuracy\n",
    "task_model_fps_results = df.groupby(['task', 'vlm', 'fps', 'method']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'accuracy': x['accuracy_of_mean'].mean(),\n",
    "        'std_err': x['accuracy_of_mean'].std() / np.sqrt(len(x))\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Get unique values\n",
    "tasks = task_model_fps_results['task'].unique()\n",
    "vlms = task_model_fps_results['vlm'].unique()\n",
    "fps_values = sorted(task_model_fps_results['fps'].unique())\n",
    "methods = task_model_fps_results['method'].unique()\n",
    "\n",
    "# Create subplot for each task\n",
    "fig, axes = plt.subplots(len(tasks), 1, figsize=(12, 5*len(tasks)))\n",
    "if len(tasks) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for task_idx, (task, ax) in enumerate(zip(tasks, axes)):\n",
    "    task_data = task_model_fps_results[task_model_fps_results['task'] == task]\n",
    "    x = np.arange(len(fps_values))\n",
    "    width = 0.8 / (len(vlms) * len(methods))  # Adjusted width for methods\n",
    "\n",
    "    # Plot bars for each VLM and method combination\n",
    "    bar_idx = 0\n",
    "    for vlm in vlms:\n",
    "        for method in methods:\n",
    "            # Calculate offset for this combination's bars\n",
    "            offset = (bar_idx - (len(vlms) * len(methods))/2 + 0.5) * width\n",
    "            \n",
    "            accuracies = []\n",
    "            errors = []\n",
    "            positions = []\n",
    "            \n",
    "            for j, fps in enumerate(fps_values):\n",
    "                combo_data = task_data[\n",
    "                    (task_data['vlm'] == vlm) & \n",
    "                    (task_data['fps'] == fps) & \n",
    "                    (task_data['method'] == method)\n",
    "                ]\n",
    "                if not combo_data.empty:\n",
    "                    positions.append(j + offset)\n",
    "                    accuracies.append(combo_data['accuracy'].iloc[0])\n",
    "                    errors.append(combo_data['std_err'].iloc[0])\n",
    "            \n",
    "            if positions:  # Only plot if we have data\n",
    "                ax.bar(positions, accuracies, width, \n",
    "                       label=f'{vlm} ({method})',\n",
    "                       alpha=0.7)\n",
    "                \n",
    "                ax.errorbar(positions, accuracies,\n",
    "                           yerr=errors, fmt='none', color='black', capsize=3)\n",
    "            \n",
    "            bar_idx += 1\n",
    "\n",
    "    # Customize each subplot\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(f'Task: {task}')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f'{fps} FPS' for fps in fps_values])\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Add horizontal line at 0.5 for random chance\n",
    "    ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.3, label='Random Chance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print best combinations\n",
    "print(\"\\nBest performing combinations for each task:\")\n",
    "for task in tasks:\n",
    "    task_data = task_model_fps_results[task_model_fps_results['task'] == task]\n",
    "    best_row = task_data.loc[task_data['accuracy'].idxmax()]\n",
    "    print(f\"\\n{task}:\")\n",
    "    print(f\"  VLM: {best_row['vlm']}\")\n",
    "    print(f\"  FPS: {best_row['fps']}\")\n",
    "    print(f\"  Method: {best_row['method']}\")\n",
    "    print(f\"  Accuracy: {best_row['accuracy']:.3f} ± {best_row['std_err']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe for vote analysis\n",
    "vote_analysis = pd.DataFrame()\n",
    "\n",
    "# Extract performance lists and analyze different numbers of votes\n",
    "for idx, row in df.iterrows():\n",
    "    perf = row['votes_float']\n",
    "    if isinstance(perf, list):\n",
    "        # For each number of votes (1, 3, 5)\n",
    "        for n_votes in [1, 3, 5]:\n",
    "            # Take first n_votes if available\n",
    "            votes = perf[:n_votes]\n",
    "            if len(votes) >= n_votes:\n",
    "                # Calculate mean performance with this many votes\n",
    "                mean_perf = np.mean(votes)\n",
    "                vote_analysis = pd.concat([vote_analysis, pd.DataFrame({\n",
    "                    'vlm': [row['vlm']],\n",
    "                    'task': [row['task']],\n",
    "                    'method': [row['method']],  # Added method\n",
    "                    'n_votes': [n_votes],\n",
    "                    'mean_performance': [mean_perf],\n",
    "                    'true_success': [1 if row['success_flag'] == 'success' else 0]\n",
    "                })])\n",
    "vote_analysis['accuracy_of_mean'] = vote_analysis['true_success'] == (vote_analysis['mean_performance'] > 0.5)\n",
    "\n",
    "# Calculate accuracy for each VLM, method, and number of votes\n",
    "vote_results = vote_analysis.groupby(['vlm', 'method', 'n_votes']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'accuracy': x['accuracy_of_mean'].mean(),\n",
    "        'std_err': x['accuracy_of_mean'].std() / np.sqrt(len(x))\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "# Create separate plots for each method\n",
    "methods = vote_results['method'].unique()\n",
    "fig, axes = plt.subplots(len(methods), 1, figsize=(12, 5*len(methods)))\n",
    "if len(methods) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for method_idx, (method, ax) in enumerate(zip(methods, axes)):\n",
    "    # Filter data for this method\n",
    "    method_data = vote_results[vote_results['method'] == method]\n",
    "    \n",
    "    # Plot each VLM for this method\n",
    "    for vlm in method_data['vlm'].unique():\n",
    "        vlm_data = method_data[method_data['vlm'] == vlm]\n",
    "        if not vlm_data.empty:\n",
    "            ax.errorbar(vlm_data['n_votes'], \n",
    "                       vlm_data['accuracy'], \n",
    "                       yerr=vlm_data['std_err'],\n",
    "                       label=vlm, \n",
    "                       marker='o', \n",
    "                       capsize=5)\n",
    "    \n",
    "    # Customize subplot\n",
    "    ax.set_xlabel('Number of Votes')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(f'Accuracy vs Number of Votes by Model ({method})')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.3, label='Random Chance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For task_results\n",
    "task_results = df.groupby(['task', 'method']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'accuracy': x['accuracy_of_mean'].mean(),\n",
    "        'std_err': x['accuracy_of_mean'].std() / np.sqrt(len(x))\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# For fps_results and related dataframes\n",
    "fps_results = df.groupby(['vlm', 'fps', 'method']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'accuracy': x['accuracy_of_mean'].mean(),\n",
    "        'std_err': x['accuracy_of_mean'].std() / np.sqrt(len(x))\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "fps_model_mean_accs = df.groupby(['vlm', 'fps', 'method']).apply(\n",
    "    lambda x: x['accuracy_of_mean'].mean()\n",
    ").reset_index(name='accuracy')\n",
    "\n",
    "fps_model_median_accs = df.groupby(['vlm', 'fps', 'method']).apply(\n",
    "    lambda x: x['accuracy_of_median'].mean()\n",
    ").reset_index(name='accuracy')\n",
    "\n",
    "# For task_model_results\n",
    "task_model_results = df.groupby(['task', 'vlm', 'method']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'accuracy': x['accuracy_of_mean'].mean(),\n",
    "        'std_err': x['accuracy_of_mean'].std() / np.sqrt(len(x))\n",
    "    })\n",
    ").reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique methods\n",
    "methods = fps_model_mean_accs['method'].unique()\n",
    "\n",
    "# Create subplot for each method\n",
    "fig, axes = plt.subplots(len(methods), 1, figsize=(10, 6*len(methods)))\n",
    "if len(methods) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Plot for each method\n",
    "for method_idx, (method, ax) in enumerate(zip(methods, axes)):\n",
    "    # Filter data for this method\n",
    "    method_mean_data = fps_model_mean_accs[fps_model_mean_accs['method'] == method]\n",
    "    method_median_data = fps_model_median_accs[fps_model_median_accs['method'] == method]\n",
    "    \n",
    "    # Plot for each VLM within this method\n",
    "    for i, vlm in enumerate(method_mean_data['vlm'].unique()):\n",
    "        # Plot mean accuracy\n",
    "        vlm_data = method_mean_data[method_mean_data['vlm'] == vlm]\n",
    "        ax.plot(vlm_data['fps'], vlm_data['accuracy'], \n",
    "                marker='o', label=f'{vlm} (mean)', linestyle='-')\n",
    "        \n",
    "        # Plot median accuracy\n",
    "        vlm_data = method_median_data[method_median_data['vlm'] == vlm]\n",
    "        ax.plot(vlm_data['fps'], vlm_data['accuracy'], \n",
    "                marker='s', label=f'{vlm} (median)', linestyle='--')\n",
    "    \n",
    "    # Customize subplot\n",
    "    ax.set_xlabel('FPS')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(f'Performance Accuracy vs FPS ({method})')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.3, label='Random Chance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per task performance\n",
    "# Group by task and calculate mean accuracy\n",
    "task_results = df.groupby(['task']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'accuracy': x['accuracy_of_mean'].mean(),\n",
    "        'std_err': x['accuracy_of_mean'].std() / np.sqrt(len(x))\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(task_results['task'], task_results['accuracy'])\n",
    "plt.errorbar(task_results['task'], task_results['accuracy'], \n",
    "             yerr=task_results['std_err'], fmt='none', color='black', capsize=5)\n",
    "\n",
    "# add horizontal line at 0.5\n",
    "plt.axhline(y=0.5, color='red', linestyle='--', label='Random Chance')\n",
    "# Customize plot\n",
    "plt.xlabel('Task')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Performance Accuracy by Task')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTask-wise Performance:\")\n",
    "print(task_results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by task and VLM to calculate mean accuracy\n",
    "task_model_results = df.groupby(['task', 'vlm']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'accuracy': x['accuracy_of_mean'].mean(),\n",
    "        'std_err': x['accuracy_of_mean'].std() / np.sqrt(len(x))\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Get unique tasks and VLMs\n",
    "# Get unique tasks and VLMs\n",
    "tasks = task_model_results['task'].unique()\n",
    "vlms = task_model_results['vlm'].unique()\n",
    "x = np.arange(len(tasks))\n",
    "width = 0.8 / len(vlms)  # Width of bars with spacing\n",
    "\n",
    "# Plot bars for each VLM\n",
    "for i, vlm in enumerate(vlms):\n",
    "    vlm_data = task_model_results[task_model_results['vlm'] == vlm]\n",
    "    # Calculate offset for this VLM's bars\n",
    "    offset = (i - len(vlms)/2 + 0.5) * width\n",
    "    # Match tasks with the current VLM's data\n",
    "    accuracies = []\n",
    "    positions = []\n",
    "    errors = []\n",
    "    \n",
    "    for task_idx, task in enumerate(tasks):\n",
    "        task_data = vlm_data[vlm_data['task'] == task]\n",
    "        if not task_data.empty:\n",
    "            positions.append(task_idx + offset)\n",
    "            accuracies.append(task_data['accuracy'].iloc[0])\n",
    "            errors.append(task_data['std_err'].iloc[0])\n",
    "    \n",
    "    plt.bar(positions, accuracies, width, label=vlm)\n",
    "    plt.errorbar(positions, accuracies,\n",
    "                yerr=errors, fmt='none', color='black', capsize=3)\n",
    "\n",
    "# Customize plot\n",
    "plt.xlabel('Task')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Performance Accuracy by Task and Model')\n",
    "plt.xticks(x, tasks, rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by task, VLM, and FPS to calculate mean accuracy\n",
    "task_model_fps_results = df.groupby(['task', 'vlm', 'fps']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'accuracy': x['accuracy_of_mean'].mean(),\n",
    "        'std_err': x['accuracy_of_mean'].std() / np.sqrt(len(x))\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Get unique values\n",
    "tasks = task_model_fps_results['task'].unique()\n",
    "vlms = task_model_fps_results['vlm'].unique()\n",
    "fps_values = sorted(task_model_fps_results['fps'].unique())\n",
    "\n",
    "# Create subplot for each task\n",
    "fig, axes = plt.subplots(len(tasks), 1, figsize=(12, 5*len(tasks)))\n",
    "if len(tasks) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for task_idx, (task, ax) in enumerate(zip(tasks, axes)):\n",
    "    task_data = task_model_fps_results[task_model_fps_results['task'] == task]\n",
    "    x = np.arange(len(fps_values))\n",
    "    width = 0.8 / len(vlms)  # Width of bars with spacing\n",
    "\n",
    "    # Plot bars for each VLM\n",
    "    for i, vlm in enumerate(vlms):\n",
    "        # Calculate offset for this VLM's bars\n",
    "        offset = (i - len(vlms)/2 + 0.5) * width\n",
    "        \n",
    "        accuracies = []\n",
    "        errors = []\n",
    "        positions = []\n",
    "        \n",
    "        for j, fps in enumerate(fps_values):\n",
    "            vlm_data = task_data[(task_data['vlm'] == vlm) & (task_data['fps'] == fps)]\n",
    "            if not vlm_data.empty:\n",
    "                positions.append(j + offset)\n",
    "                accuracies.append(vlm_data['accuracy'].iloc[0])\n",
    "                errors.append(vlm_data['std_err'].iloc[0])\n",
    "        \n",
    "        ax.bar(positions, accuracies, width, \n",
    "               label=vlm,\n",
    "               alpha=0.7)\n",
    "        \n",
    "        ax.errorbar(positions, accuracies,\n",
    "                   yerr=errors, fmt='none', color='black', capsize=3)\n",
    "\n",
    "    # Customize each subplot\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(f'Task: {task}')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f'{fps} FPS' for fps in fps_values])\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add horizontal line at 0.5 for random chance\n",
    "    ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.3, label='Random Chance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print best combinations\n",
    "print(\"\\nBest performing combinations for each task:\")\n",
    "for task in tasks:\n",
    "    task_data = task_model_fps_results[task_model_fps_results['task'] == task]\n",
    "    best_row = task_data.loc[task_data['accuracy'].idxmax()]\n",
    "    print(f\"\\n{task}:\")\n",
    "    print(f\"  VLM: {best_row['vlm']}\")\n",
    "    print(f\"  FPS: {best_row['fps']}\")\n",
    "    print(f\"  Accuracy: {best_row['accuracy']:.3f} ± {best_row['std_err']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall method performance\n",
    "method_results = df.groupby(['method']).agg({\n",
    "    'accuracy_of_mean': ['mean', lambda x: x.std() / np.sqrt(len(x))]\n",
    "}).reset_index()\n",
    "method_results.columns = ['method', 'accuracy', 'std_err']\n",
    "\n",
    "# Method performance by model\n",
    "model_method_results = df.groupby(['vlm', 'method']).agg({\n",
    "    'accuracy_of_mean': ['mean', lambda x: x.std() / np.sqrt(len(x))]\n",
    "}).reset_index()\n",
    "model_method_results.columns = ['vlm', 'method', 'accuracy', 'std_err']\n",
    "\n",
    "# Create two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Overall method performance\n",
    "ax1.bar(method_results['method'], method_results['accuracy'])\n",
    "ax1.errorbar(method_results['method'], method_results['accuracy'], \n",
    "             yerr=method_results['std_err'], fmt='none', color='black', capsize=5)\n",
    "ax1.set_title('Overall Method Performance')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=0.5, color='red', linestyle='--', alpha=0.3, label='Random Chance')\n",
    "\n",
    "# Plot 2: Method performance by model\n",
    "vlms = model_method_results['vlm'].unique()\n",
    "methods = model_method_results['method'].unique()\n",
    "x = np.arange(len(vlms))\n",
    "width = 0.35\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    method_data = model_method_results[model_method_results['method'] == method]\n",
    "    offset = (i - len(methods)/2 + 0.5) * width\n",
    "    \n",
    "    # Ensure data aligns with x-axis positions\n",
    "    accuracies = []\n",
    "    errors = []\n",
    "    for vlm in vlms:\n",
    "        vlm_data = method_data[method_data['vlm'] == vlm]\n",
    "        if not vlm_data.empty:\n",
    "            accuracies.append(vlm_data['accuracy'].iloc[0])\n",
    "            errors.append(vlm_data['std_err'].iloc[0])\n",
    "        else:\n",
    "            accuracies.append(0)\n",
    "            errors.append(0)\n",
    "    \n",
    "    ax2.bar(x + offset, accuracies, width, label=method)\n",
    "    ax2.errorbar(x + offset, accuracies, \n",
    "                yerr=errors, fmt='none', color='black', capsize=3)\n",
    "\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Method Performance by Model')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(vlms, rotation=45)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0.5, color='red', linestyle='--', alpha=0.3, label='Random Chance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print numerical results\n",
    "print(\"\\nOverall Method Performance:\")\n",
    "print(method_results.to_string(index=False))\n",
    "print(\"\\nMethod Performance by Model:\")\n",
    "print(model_method_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
