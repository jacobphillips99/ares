{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potentially combine paths!\n",
    "paths = [\n",
    "    \"/tmp/eval_results_2024-12-26_19-26-57.csv\",\n",
    "    \"/tmp/eval_results_2024-12-26_19-58-11.csv\",\n",
    "    \"/tmp/eval_results_2024-12-26_20-27-29.csv\",\n",
    "    \"/tmp/eval_results_2024-12-26_20-42-07.csv\",\n",
    "    \"/tmp/eval_results_gpt-4o-mini_2024-12-26_23-38-09.csv\",\n",
    "    \"/tmp/eval_results_gpt-4o_2024-12-27_01-42-46.csv\",\n",
    "    \"/tmp/eval_results_gemini-2.0-flash-exp_2024-12-27_02-14-20.csv\",\n",
    "    \"/tmp/eval_results_gemini-1.5-pro_2024-12-27_03-01-14.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv(path) for path in paths])\n",
    "df['label'] = df['success_flag'].apply(lambda x: 1 if x== 'success' else 0)\n",
    "df['accuracy_of_mean'] = df['label'] == (df['mean_performance'] > 0.5)\n",
    "df['accuracy_of_median'] = df['label'] == (df['median_performance'] > 0.5)\n",
    "\n",
    "df['vote_str'] = df['performance']\n",
    "df['votes_float'] = df['performance'].apply(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['accuracy_of_mean'].mean(), df['accuracy_of_median'].mean(), df['votes_float'].apply(lambda x: len(x)).mean())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for computing means and std errors\n",
    "def mean_stderr(data):\n",
    "    mean = np.mean(data)\n",
    "    stderr = np.std(data) / np.sqrt(len(data))\n",
    "    return mean, stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance of mean vs median for each model \n",
    "# Group by VLM and compute accuracy for both mean and median methods\n",
    "grouped_acc = df.groupby('vlm').agg({\n",
    "    'accuracy_of_mean': ['mean', 'std'],\n",
    "    'accuracy_of_median': ['mean', 'std']\n",
    "}).round(3)\n",
    "\n",
    "# Create bar plot comparing mean vs median performance\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(len(grouped_acc.index))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, grouped_acc['accuracy_of_mean']['mean'], width, \n",
    "        label='Mean threshold', \n",
    "        yerr=grouped_acc['accuracy_of_mean']['std'],\n",
    "        capsize=5)\n",
    "plt.bar(x + width/2, grouped_acc['accuracy_of_median']['mean'], width,\n",
    "        label='Median threshold',\n",
    "        yerr=grouped_acc['accuracy_of_median']['std'], \n",
    "        capsize=5)\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Performance Metric Comparison by Model')\n",
    "plt.xticks(x, grouped_acc.index, rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, what about as we change the number of votes? 1 vs 3 vs 5? get the votes from the 'performance' column. some are nan / not full length, just reuse\n",
    "\n",
    "# Create a new dataframe for vote analysis\n",
    "vote_analysis = pd.DataFrame()\n",
    "\n",
    "# Extract performance lists and analyze different numbers of votes\n",
    "for idx, row in df.iterrows():\n",
    "    perf = row['votes_float']\n",
    "    if isinstance(perf, list):\n",
    "        # For each number of votes (1, 3, 5)\n",
    "        for n_votes in [1, 3, 5]:\n",
    "            # Take first n_votes if available\n",
    "            votes = perf[:n_votes]\n",
    "            if len(votes) >= n_votes:\n",
    "                # Calculate mean performance with this many votes\n",
    "                mean_perf = np.mean(votes)\n",
    "                vote_analysis = pd.concat([vote_analysis, pd.DataFrame({\n",
    "                    'vlm': [row['vlm']],\n",
    "                    'task': [row['task']],\n",
    "                    'n_votes': [n_votes],\n",
    "                    'mean_performance': [mean_perf],\n",
    "                    'true_success': [1 if row['success_flag'] == 'success' else 0]\n",
    "                })])\n",
    "vote_analysis['accuracy_of_mean'] = vote_analysis['true_success'] == (vote_analysis['mean_performance'] > 0.5)\n",
    "\n",
    "# Calculate accuracy for each VLM and number of votes\n",
    "vote_results = vote_analysis.groupby(['vlm', 'n_votes']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'accuracy': x['accuracy_of_mean'].mean(),\n",
    "        'std_err': x['accuracy_of_mean'].std() / np.sqrt(len(x))\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "for vlm in vote_results['vlm'].unique():\n",
    "    vlm_data = vote_results[vote_results['vlm'] == vlm]\n",
    "    plt.errorbar(vlm_data['n_votes'], vlm_data['accuracy'], \n",
    "                yerr=vlm_data['std_err'],\n",
    "                label=vlm, marker='o', capsize=5)\n",
    "\n",
    "plt.xlabel('Number of Votes')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Number of Votes by Model')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate accuracy for each VLM and FPS\n",
    "fps_results = df.groupby(['vlm', 'fps']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'accuracy': x['accuracy_of_mean'].mean(),\n",
    "        'std_err': x['accuracy_of_mean'].std() / np.sqrt(len(x))\n",
    "    })\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps_model_mean_accs = df.groupby(['vlm', 'fps']).apply(\n",
    "    lambda x: x['accuracy_of_mean'].mean()\n",
    ").reset_index(name='accuracy')\n",
    "\n",
    "fps_model_median_accs = df.groupby(['vlm', 'fps']).apply(\n",
    "    lambda x: x['accuracy_of_median'].mean()\n",
    ").reset_index(name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot both mean and median accuracy on same plot\n",
    "for vlm in fps_model_mean_accs['vlm'].unique():\n",
    "    # Plot mean accuracy\n",
    "    vlm_data = fps_model_mean_accs[fps_model_mean_accs['vlm'] == vlm]\n",
    "    plt.plot(vlm_data['fps'], vlm_data['accuracy'], marker='o', label=f'{vlm} (mean)', linestyle='-')\n",
    "    \n",
    "    # Plot median accuracy\n",
    "    vlm_data = fps_model_median_accs[fps_model_median_accs['vlm'] == vlm]\n",
    "    plt.plot(vlm_data['fps'], vlm_data['accuracy'], marker='s', label=f'{vlm} (median)', linestyle='--')\n",
    "\n",
    "plt.xlabel('FPS')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Performance Accuracy vs FPS')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per task performance\n",
    "# Group by task and calculate mean accuracy\n",
    "task_results = df.groupby(['task']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'accuracy': x['accuracy_of_mean'].mean(),\n",
    "        'std_err': x['accuracy_of_mean'].std() / np.sqrt(len(x))\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(task_results['task'], task_results['accuracy'])\n",
    "plt.errorbar(task_results['task'], task_results['accuracy'], \n",
    "             yerr=task_results['std_err'], fmt='none', color='black', capsize=5)\n",
    "\n",
    "# Customize plot\n",
    "plt.xlabel('Task')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Performance Accuracy by Task')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTask-wise Performance:\")\n",
    "print(task_results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perforance by task by model \n",
    "# Group by task and VLM to calculate mean accuracy\n",
    "task_model_results = df.groupby(['task', 'vlm']).apply(\n",
    "    lambda x: pd.Series({\n",
    "        'accuracy': x['accuracy_of_mean'].mean(),\n",
    "        'std_err': x['accuracy_of_mean'].std() / np.sqrt(len(x))\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Create grouped bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Get unique tasks and VLMs\n",
    "tasks = task_model_results['task'].unique()\n",
    "vlms = task_model_results['vlm'].unique()\n",
    "x = np.arange(len(tasks))\n",
    "width = 0.8 / len(vlms)  # Width of bars with spacing\n",
    "\n",
    "# Plot bars for each VLM\n",
    "for i, vlm in enumerate(vlms):\n",
    "    vlm_data = task_model_results[task_model_results['vlm'] == vlm]\n",
    "    offset = (i - len(vlms)/2 + 0.5) * width\n",
    "    bars = plt.bar(x + offset, vlm_data['accuracy'], width, label=vlm)\n",
    "    plt.errorbar(x + offset, vlm_data['accuracy'],\n",
    "                yerr=vlm_data['std_err'], fmt='none', color='black', capsize=3)\n",
    "\n",
    "# Customize plot\n",
    "plt.xlabel('Task')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Performance Accuracy by Task and Model')\n",
    "plt.xticks(x, tasks, rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTask and Model-wise Performance:\")\n",
    "print(task_model_results.to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
