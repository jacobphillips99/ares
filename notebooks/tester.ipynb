{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import os\n",
    "import logging\n",
    "from openvla.prismatic.vla.datasets import (\n",
    "    RLDSDataset,\n",
    "    EpisodicRLDSDataset,\n",
    "    RLDSBatchTransform,\n",
    ")\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "from transformers import AutoConfig, AutoImageProcessor\n",
    "from openvla.prismatic.models.backbones.llm.prompting import (\n",
    "    PurePromptBuilder,\n",
    "    VicunaV15ChatPromptBuilder,\n",
    ")\n",
    "from openvla.prismatic.vla.action_tokenizer import ActionTokenizer\n",
    "\n",
    "\n",
    "hf_base = \"jxu124/OpenX-Embodiment\"\n",
    "ucsd_kitchen_dataset_name = \"ucsd_kitchen_dataset_converted_externally_to_rlds\"\n",
    "cmu_play_fusion_dataset_name = \"cmu_play_fusion\"\n",
    "# Load the dataset with the custom session\n",
    "# ds = datasets.load_dataset(\n",
    "#     hf_base,\n",
    "#     ucsd_kitchen_dataset_name,\n",
    "#     split=\"train\",\n",
    "#     cache_dir=\"/workspaces/ares/data\",\n",
    "# )\n",
    "# breakpoint()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EpisodicConfig:\n",
    "    data_root_dir: Path = Path(\n",
    "        # \"datasets/open-x-embodiment\"\n",
    "        f\"/workspaces/ares/data\"\n",
    "    )  # Path to Open-X dataset directory\n",
    "    # dataset_name: str = ucsd_kitchen_dataset_name\n",
    "    dataset_name: str = cmu_play_fusion_dataset_name\n",
    "    image_sizes: tuple[int] = (224, 224)\n",
    "    vla_path: str = \"openvla/openvla-7b\"  # Path to OpenVLA model (on HuggingFace Hub)\n",
    "    shuffle_buffer_size: int = 256  # _000\n",
    "    image_aug: bool = False\n",
    "\n",
    "\n",
    "cfg = EpisodicConfig()\n",
    "processor = AutoProcessor.from_pretrained(cfg.vla_path, trust_remote_code=True)\n",
    "action_tokenizer = ActionTokenizer(processor.tokenizer)\n",
    "\n",
    "# batch_transform = RLDSBatchTransform(\n",
    "#     action_tokenizer,\n",
    "#     processor.tokenizer,\n",
    "#     image_transform=processor.image_processor.apply_transform,\n",
    "#     prompt_builder_fn=(\n",
    "#         PurePromptBuilder if \"v01\" not in cfg.vla_path else VicunaV15ChatPromptBuilder\n",
    "#     ),\n",
    "# )\n",
    "batch_transform = lambda x: x\n",
    "vla_dataset = EpisodicRLDSDataset(\n",
    "    cfg.data_root_dir,\n",
    "    cfg.dataset_name,\n",
    "    batch_transform,\n",
    "    resize_resolution=tuple(cfg.image_sizes),\n",
    "    shuffle_buffer_size=cfg.shuffle_buffer_size,\n",
    "    image_aug=cfg.image_aug,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlds_batch_episode = next(iter(vla_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = next(iter(vla_dataset.dataset.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out['observation']['image_primary'].shape)\n",
    "out['observation']['timestep'].T, out['task']['timestep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task, dataset_name = rlds_batch_i[\"task\"], rlds_batch_i[\"dataset_name\"]\n",
    "# # print(dataset_name)\n",
    "# # print(\"task:   \")\n",
    "# # # lang instruction, image primary, timestemp, mask\n",
    "# for k,v in task.items():\n",
    "#     print(\"\\t\", k, type(v))\n",
    "\n",
    "# # print(f\"action: {rlds_batch_i['action'][0]}\")\n",
    "# from PIL import Image\n",
    "# img_vals = rlds_batch_i[\"observation\"][\"image_primary\"][0]\n",
    "# print(type(img_vals), img_vals.shape, img_vals.min(), img_vals.max())\n",
    "# img = Image.fromarray(img_vals)\n",
    "# lang_instruction = rlds_batch_i[\"task\"][\"language_instruction\"].decode().lower()\n",
    "# timestep = rlds_batch_i[\"task\"][\"timestep\"]\n",
    "\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(img)\n",
    "# plt.title(f\"{lang_instruction}\\n{timestep}/{len(rlds_batch_episode)}\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "\n",
    "def create_episode_gif(rlds_batch_episode, output_path='episode.gif', frame_duration=500):\n",
    "    \"\"\"\n",
    "    Creates a GIF from RLDS episode data with properly rendered captions in a Jupyter notebook.\n",
    "    \n",
    "    Args:\n",
    "        rlds_batch_episode: Batch of RLDS episode data\n",
    "        output_path: Path to save the output GIF\n",
    "        frame_duration: Duration for each frame in milliseconds\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    \n",
    "    # Clear any existing plots and set up the matplotlib backend\n",
    "    plt.close('all')\n",
    "    %matplotlib inline\n",
    "    \n",
    "    print(\"Creating GIF...\")\n",
    "    \n",
    "    for i, step in enumerate(rlds_batch_episode):\n",
    "        # Show progress\n",
    "        display.clear_output(wait=True)\n",
    "        print(f\"Processing frame {i+1}/{len(rlds_batch_episode)}...\")\n",
    "        \n",
    "        img = Image.fromarray(step[\"observation\"][\"image_primary\"][0])\n",
    "        lang_instruction = step[\"task\"][\"language_instruction\"].decode().lower()\n",
    "        # timestep = step[\"task\"][\"timestep\"]\n",
    "        timestep = step[\"observation\"][\"timestep\"]\n",
    "        \n",
    "        # Create figure with adequate size and margins\n",
    "        fig = plt.figure(figsize=(10, 8), dpi=100)\n",
    "        plt.subplots_adjust(top=0.85)\n",
    "        \n",
    "        # Display the image\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Add title with proper wrapping\n",
    "        plt.title(f\"{lang_instruction}\\n{timestep}/{len(rlds_batch_episode)}\", \n",
    "                 wrap=True, \n",
    "                 pad=20,\n",
    "                 fontsize=12)\n",
    "        \n",
    "        # Convert to image\n",
    "        fig.canvas.draw()\n",
    "        img_with_title = Image.frombytes('RGB', \n",
    "                                       fig.canvas.get_width_height(),\n",
    "                                       fig.canvas.tostring_rgb())\n",
    "        \n",
    "        images.append(img_with_title)\n",
    "        plt.close(fig)  # Important in Jupyter to prevent memory leaks\n",
    "    \n",
    "    # Save as animated GIF\n",
    "    images[0].save(\n",
    "        output_path,\n",
    "        save_all=True,\n",
    "        append_images=images[1:],\n",
    "        duration=frame_duration,\n",
    "        loop=0\n",
    "    )\n",
    "    \n",
    "    # Clear the progress output and show completion message\n",
    "    display.clear_output(wait=True)\n",
    "    print(\"GIF created successfully!\")\n",
    "    \n",
    "    # Display the final GIF\n",
    "    return display.Image(filename=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vla_dataset.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_ds = iter(vla_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlds_batch_episode = next(iter_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the gif\n",
    "create_episode_gif(rlds_batch_episode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
