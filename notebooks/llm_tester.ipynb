{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import base64\n",
    "import os\n",
    "import typing as t\n",
    "\n",
    "import numpy as np\n",
    "import vertexai\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "from litellm import completion, completion_cost\n",
    "from litellm.utils import ModelResponse\n",
    "from PIL import Image\n",
    "from pydantic import BaseModel, Field\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "\n",
    "from ares.configs.base import pydantic_to_example_dict, pydantic_to_field_instructions\n",
    "from ares.utils.image_utils import (\n",
    "    choose_and_preprocess_frames,\n",
    "    encode_image,\n",
    "    split_video_to_frames,\n",
    ")\n",
    "from ares.extras.pi_demo_utils import PI_DEMO_PATH, PI_DEMO_TASKS\n",
    "\n",
    "class Nested(BaseModel):\n",
    "    the_nested_attr: str\n",
    "    other_nested_attr: str\n",
    "\n",
    "class RolloutDescription(BaseModel):\n",
    "    nested: Nested\n",
    "    robot_setup: t.Literal[\"one arm\", \"two arms\"]\n",
    "    environment: t.Literal[\"floor\", \"table\", \"other\"]\n",
    "    lighting_conditions: t.Literal[\"normal\", \"dim\", \"bright\"]\n",
    "    # task: str = Field(max_length=50, description=\"Short task description\")\n",
    "    description: str = Field(\n",
    "        max_length=1000,\n",
    "        description=\"A detailed description of the robot's actions over the course of the images. Don't include fluff like 'Let's describe...'. Just describe the episode.\",\n",
    "    )\n",
    "    success_str: str = Field(\n",
    "        max_length=1000,\n",
    "        description=\"\"\"\n",
    "    A detailed description of whether or not the robot successfully completes the task. \n",
    "    Be very specific and critical about whether or not the robot has met the intended goal state of the task and include lots of details pertaining to partial success.\n",
    "    In order to be successful, the robot must have completed the task in a way that is consistent with the task description. Any error or deviation from the task description is a failure.\n",
    "    \"\"\".strip(),\n",
    "    )\n",
    "    success_score: float = Field(\n",
    "        description=\"A float score between 0 and 1, representing the success of the task. A score of 0 means the task was not completed at all, and a score of 1 means the task was completed absolutely perfectly.\",\n",
    "    )\n",
    "\n",
    "# Build instruction string dynamically from model fields\n",
    "field_instructions = pydantic_to_field_instructions(RolloutDescription)\n",
    "\n",
    "# Build instructions string, will go into prompt jinja2 template\n",
    "instructions = \"\"\"\n",
    "Look at the images provided and consider the following task description:\n",
    "TASK: {task}\n",
    "\n",
    "Create a response to the task by answering the following questions:\n",
    "{field_instructions}\n",
    "\"\"\".strip()\n",
    "\n",
    "# Build example response dict dynamically from model fields\n",
    "response_format = f\"\"\"\n",
    "For the response, first respond with about 500 words that describe the entire video, focusing on the robot's actions and the task.\n",
    "Then, respond with a python dict, e.g. {pydantic_to_example_dict(RolloutDescription)} that fulfills the above specifications.\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ares.image_utils import split_video_to_frames, choose_and_preprocess_frames\n",
    "\n",
    "\n",
    "def get_frames(task, success, n_frames: t.Optional[int] = None):\n",
    "    video_path = os.path.join(\n",
    "        PI_DEMO_PATH, f\"{PI_DEMO_TASKS[task]['filename_prefix']}_{success}.mp4\"\n",
    "    )\n",
    "    all_frames = split_video_to_frames(video_path)\n",
    "    print(f\"split video into {len(all_frames)} frames\")\n",
    "    specified_frames: list[int] | None = None\n",
    "    frames = choose_and_preprocess_frames(\n",
    "        all_frames, n_frames if n_frames else len(all_frames), specified_frames=specified_frames, resize=(512, 512)\n",
    "    )\n",
    "    return frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# os.environ[\"LITELLM_LOG\"] = \"DEBUG\"\n",
    "# litellm.set_verbose=True\n",
    "# task = \"Eggs in carton\"\n",
    "# task = \"Grocery Bagging\"\n",
    "# task = \"Toast out of toaster\"\n",
    "# task = \"Towel fold\"\n",
    "# task = \"Stack bowls\"\n",
    "# task = \"Tupperware in microwave\"\n",
    "# task = \"Items in drawer\"\n",
    "# task = \"Laundry fold (shirts)\"\n",
    "# task = \"Laundry fold (shorts)\"\n",
    "task = \"Paper towel in holder\"\n",
    "# task = \"Food in to go box\"\n",
    "success = \"fail\"\n",
    "# success = \"success\"\n",
    "\n",
    "\n",
    "# provider = \"gemini\"\n",
    "# name = f\"{provider}/gemini-1.5-flash\"\n",
    "\n",
    "provider = \"openai\"\n",
    "name = f\"{provider}/gpt-4o\"\n",
    "# name = f\"{provider}/gpt-4o-mini\"\n",
    "# name = f\"{provider}/gpt-4-turbo\"\n",
    "\n",
    "# provider = \"anthropic\"\n",
    "# name = f\"{provider}/claude-3-5-sonnet-20240620\"\n",
    "\n",
    "from ares.models.base import VLM\n",
    "\n",
    "# vlm = GeminiVideoVLM(\"gemini\", \"gemini-1.5-flash\", dict())\n",
    "vlm = VLM(provider=provider, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frames = get_frames(task, success, n_frames=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffs = [np.mean(np.abs(np.array(all_frames[i]) - np.array(all_frames[i+1]))) for i in range(len(all_frames) -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import CLIPProcessor, CLIPModel\n",
    "# model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# def get_embedding(img):\n",
    "#     with torch.no_grad():\n",
    "#         inputs = processor(text=[\"a picture\"], images=img, return_tensors=\"pt\", padding=True)\n",
    "#         outputs = model(**inputs)\n",
    "#     return outputs['image_embeds'].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeds = [get_embedding(all_frames[i]) for i in range(len(all_frames)) if i% 5 == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cosine_similarity(a, b):\n",
    "#     \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "#     dot_product = np.dot(a.flatten(), b.flatten())\n",
    "#     norm_a = np.linalg.norm(a)\n",
    "#     norm_b = np.linalg.norm(b)\n",
    "#     return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# angles = [cosine_similarity(embeds[i], embeds[i+1]) for i in range(len(embeds)-1)]  # Calculate cosine similarity with other embeddings\n",
    "# # find the defivative of changes\n",
    "# # Calculate the derivative (rate of change) of angles\n",
    "# # angle_changes = np.diff(angles)\n",
    "# angle_changes = np.gradient(angles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the angles \n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 4))\n",
    "\n",
    "# ax1.scatter(range(len(angles)), angles, s=10)\n",
    "# ax1.set_xlabel('Frame Index')\n",
    "# ax1.set_ylabel('CLIP Cosine Similarity to Next Frame')\n",
    "# ax1.set_title('Similarity per Frame')\n",
    "\n",
    "# ax2.scatter(range(len(angle_changes)), angle_changes, s=10)\n",
    "# ax2.set_xlabel('Frame Index')\n",
    "# ax2.set_ylabel('Angles derivative')\n",
    "# ax2.set_title('Angles derivative')\n",
    "\n",
    "\n",
    "# ax3.hist(angles, bins=30, edgecolor='black')\n",
    "# ax3.set_xlabel('CLIP Cosine Similarity')\n",
    "# ax3.set_ylabel('Frequency')\n",
    "# ax3.set_title('Distribution of Frame Differences')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i want a heatmap showing similarity of all to all\n",
    "# Create similarity matrix of all embeddings compared to all other embeddings\n",
    "# n = len(embeds)\n",
    "# similarity_matrix = np.zeros((n, n))\n",
    "# for i in range(n):\n",
    "#     for j in range(n):\n",
    "#         similarity_matrix[i,j] = cosine_similarity(embeds[i], embeds[j])\n",
    "\n",
    "# # Plot heatmap\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.imshow(similarity_matrix, cmap='viridis')\n",
    "# plt.colorbar(label='Cosine Similarity')\n",
    "# plt.xlabel('Frame Index')\n",
    "# plt.ylabel('Frame Index') \n",
    "# plt.title('All-to-All Frame Similarity Matrix')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# angles = np.array(angles)\n",
    "# angles[angles <.96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples = np.where(np.array(angle_changes) < -.01)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(len(examples), 2, figsize=(20,20))\n",
    "# for i, ex in enumerate(examples):\n",
    "#     ax[i][0].imshow(all_frames[5*(ex)])\n",
    "#     ax[i][1].imshow(all_frames[5*(ex+1)])\n",
    "#     # title the row with the similarity\n",
    "#     ax[i][0].set_title(f'Frame {5*ex}')\n",
    "#     ax[i][1].set_title(f'Frame {5*(ex+1)}, Similarity: {angles[ex]:.3f}')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# ax1.scatter(range(len(diffs)), diffs, s=10)\n",
    "# ax1.set_xlabel('Frame Index')\n",
    "# ax1.set_ylabel('Mean Absolute Difference')\n",
    "# ax1.set_title('Diff per Frame')\n",
    "\n",
    "# ax2.hist(diffs, bins=30, edgecolor='black')\n",
    "# ax2.set_xlabel('Abs Frame Difference')\n",
    "# ax2.set_ylabel('Frequency')\n",
    "# ax2.set_title('Distribution of Frame Differences')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_messages(messages):\n",
    "    images = []\n",
    "    for m in messages:\n",
    "        print(m['role'])\n",
    "        contents = m['content']\n",
    "        for content in contents: \n",
    "            if content['type'] == 'text': \n",
    "                print(content['text'])\n",
    "            elif content['type'] == 'image_url':\n",
    "                byte_image = content['image_url']['url'][len('data:image/jpeg;base64,'):]\n",
    "                img_data = base64.b64decode(byte_image)\n",
    "                img = Image.open(io.BytesIO(img_data))\n",
    "                images.append(img)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    # display grid of images \n",
    "    # Calculate grid dimensions\n",
    "    n = len(images)\n",
    "    if n == 0:\n",
    "        return\n",
    "    \n",
    "    cols = int(np.ceil(np.sqrt(n)))\n",
    "    rows = int(np.ceil(n / cols))\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    for i, img in enumerate(images):\n",
    "        ax = fig.add_subplot(rows, cols, i+1)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dict = {\n",
    "    \"instructions\": instructions.format(task=PI_DEMO_TASKS[task]['task'], field_instructions=chr(10).join(field_instructions)),\n",
    "    \"response_format\": response_format,\n",
    "}\n",
    "frames = get_frames(task, success, n_frames=10)\n",
    "\n",
    "messages, res = vlm.ask(\n",
    "    \"extractor_prompt.jinja2\",\n",
    "    info_dict,\n",
    "    images=frames,\n",
    "    double_prompt=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.choices[0].message.content, completion_cost(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_list = \"\"\"\n",
    "1. paper towel roll is vertically aligned with and fully inserted onto the spindle.\n",
    "2. roll sits securely within the holder frame.\n",
    "3. no significant lateral gap between the roll's core and spindle.\n",
    "4. spindle is not empty; paper towel roll is no longer on the table.\n",
    "5. no visible collisions with the table or objects during the task.\n",
    "6. roll remains intact and upright after placement.\n",
    "\"\"\"\n",
    "\n",
    "info_dict = {\n",
    "    # \"instructions\": instructions.format(task=PI_DEMO_TASKS[task]['task'], field_instructions=chr(10).join(field_instructions)),\n",
    "    # \"response_format\": response_format,\n",
    "    \"instructions\": f\"Tell me if the TASK: `{PI_DEMO_TASKS[task]['task']}` has been completed according to the constraints list: {constraints_list}. Tell me why or why not.\",\n",
    "    \"response_format\": \"respond in a single string\"\n",
    "}\n",
    "frames = get_frames(task, success, n_frames=10)\n",
    "\n",
    "outputs = []\n",
    "for i in range(10):\n",
    "    messages, res = vlm.ask(\n",
    "        \"extractor_prompt.jinja2\",\n",
    "        info_dict,\n",
    "        images=[frames[i]],\n",
    "        double_prompt=True,\n",
    "    )\n",
    "    outputs.append(res.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dict = {\n",
    "   \"instructions\": f\"summarize the following text, paying extra attention towards the end. each line represents an answer about a frame of video, in order. {str(outputs)}\",\n",
    "   \"response_format\": \"reply in a single string\"\n",
    "}\n",
    "\n",
    "messages, res = vlm.ask(\n",
    "    \"extractor_prompt.jinja2\",\n",
    "    info_dict,\n",
    "    images=[],\n",
    "    # double_prompt=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.choices[0].message.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
